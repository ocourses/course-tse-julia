{
  "hash": "dc3af714a96d1ec6832fbccaaf888de0",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Type stability for performance\"\npage-layout: article\njupyter: julia-1.11\n---\n\n\n\nThis page provides in introduction to **performance** enhancement in Julia. First, we’ll take a look at the fundamentals of computer hardware, understanding how processors and memory impact the speed of your code. Once we grasp these hardware concepts, we’ll explore how type-stability in Julia can significantly enhance performance by enabling the compiler to generate more efficient machine code.\n\n## Simplified hardware\n\n*(Ref. Jakob Nissen, [Hardware for scientists](https://viralinstruction.com/posts/hardware/))*\n\nBefore we dive into the *software* side of performance, let us gain a basic understanding of computer *hardware*. The following schematic provides a simplified representation of a computer. \n\n$$[CPU] \\Leftrightarrow [RAM] \\Leftrightarrow [DISK]$$\n\nIn this figure, where the arrows represent bidirectional data flows, the three important components are:\n\n* The central processing unit (**CPU**) where all the computations occur;\n* Random access memory (**RAM**) which is a temporary memory. Data stored here is lost in case of power failure;\n* The **disk** which is the persistent storage of the computer. \n\nThe operations executed by the CPU are coordinated by its internal clock. A *clock cycle* refers to the time required to execute a basic operation. To be executed a program has to be broken down into CPU instructions, a set of operations. This translation is done by the *compiler*. Any optimisation performed at compile time allows to generate more efficient CPU instructions, which speeds up the program runtime. \n\n::: {.callout-tip icon=false}\n\n## Remember\n\nTo run, any program has to be compiled to CPU instructions. \n\n:::\n\nThe execution of a program requires data stored in the RAM. Note that the number of clock cycles required to transfer data from the RAM to the CPU is on the order of magnitude of 500 clock cycles. \n\n::: {.callout-tip icon=false}\n\n## Remember\n\nData transfers from RAM to CPU are slow.\n\n:::\n\nIn order to limit data transfers between the CPU and RAM, there are intermediate storage spaces with, however, limited capacity. Closest to the CPU compute cores are the registers which store very limited data compared to the various levels of cache further away. The simplified computer schematic can be updated. \n\n$$[CPU] \\Leftrightarrow [Registers] \\Leftrightarrow [Caches] \\Leftrightarrow [RAM] \\Leftrightarrow [DISK]$$\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## The documentation of Julia mentions \"Multidimensional arrays in Julia are stored in column-major order\". We wish to sum the elements of a matrix. How to write a cache-efficient program to do this computation?\n\nIn column-major order, the elements of each column are stored contiguously in memory. When the CPU accesses data, it pulls in cache lines (typically 64 bytes) from RAM into cache. So access patterns matter a lot.\n\n##### Cache-Friendly Access\n\n::: {#fe0c782a .cell execution_count=2}\n``` {.julia .cell-code}\nsum = 0.\nfor j in 1:n\n    for i in 1:m\n        sum = sum + A[i,j]\n    end\nend\n```\n:::\n\n\n##### Cache-Unfriendly Access\n\n::: {#57306f10 .cell execution_count=3}\n``` {.julia .cell-code}\nsum = 0.\nfor i in 1:m\n    for j in 1:n\n        sum = sum + A[i,j]\n    end\nend\n```\n:::\n\n\nEach inner loop iteration skips over columns, which are non-contiguous in memory. This induces so called cache misses. \n:::\n\n## Type-stability\n\n*(Ref. Chris Rackauckas, [Parallel Computing and Scientific Machine Learning (SciML): Methods and Applications](https://book.sciml.ai/notes/02-Optimizing_Serial_Code/))*\n\nCommon knowledge has it that Julia is fast because it is Just-in-Time (JIT) compiled. The actual reason is the *type inference* algorithm executed before the compiler combined with *type specialization* in functions. \n\nRecall the *hardware* point of view. Let a CPU instruction read a value from a register. *How to interpret the memory of that value?* \n\nThis is straight forward if the instruction has the type information. Take a 64 bit integer and a 64 bit floating point number. They require the same memory space. However, the associated memory chunck is interpreted differently. The binary representation of that integer is `[sign bit][value in two's complement on 63 bits]` and `[sign bit][11 exponent bits][52 mantissa bits]` for the floating point number. This is what happens when writing code in the C or Fortran languages, which require explicit typing.\n\n::: {.callout-note collapse=\"true\"}\n## Number representation\n\nLet $b \\geq 2$ be a base used for numerical encoding. Any positive number can be expressed in this base as a sum of the form  \n$$\n\\sum_{k=0}^{q} c_k b^k.\n$$  \n\nComputers typically use **binary encoding**, where the base $b$ is $2$. The coefficients $c_k$, known as *bits* (and often grouped in sets of 8 called *bytes*), represent the digits in this base.\n\n**Integers** \n\nWhen encoding an integer in binary using 64 bits, the first bit indicates the **sign**:\n\n- `0` represents a non-negative (positive or zero) number  \n- `1` represents a negative number  \n\nThe remaining 63 bits encode the absolute value of the integer using the binary representation of its coefficients $c_k$.\n\n**Floating Point Numbers**\n\nFloating-point number encoding was standardized in 1985 by the IEEE 754 specification. In this format, a number $x$ is represented as:\n\n$$\nx = (-1)^s \\times M \\times 2^e\n$$\n\nwhere:\n- $s$ is the **sign bit** (0 for positive, 1 for negative),\n- $M$ is the **mantissa** (also called the significand),\n- $e$ is the **exponent**.\n\n*Example: Encoding $x = 6.5$*\n\nFirst, we convert $6.5$ into binary:\n- $6 = 4 + 2 = 2^2 + 2^1$, so in binary: `110`\n- $0.5 = 2^{-1}$, so in binary: `0.1`\n\nThus, $6.5$ in binary is `110.1`.\n\nTo express this in **normalized binary form**, we rewrite it as:\n\n$$\n6.5 = 1.101 \\times 2^2\n$$\n\nHere, the exponent $e = 2$, and the mantissa $M = 1.101$.\n\nIEEE 754 uses a **bias** to store only non-negative exponent values. For double-precision (64-bit) floating-point numbers:\n- The exponent is stored using 11 bits,\n- The bias is 1023.\n\nSo the encoded exponent is:\n\n$$\ne_{\\text{encoded}} = e + \\text{bias} = 2 + 1023 = 1025\n$$\n\nThis value (1025) is written in binary and stored in the 11-bit exponent field.\n\nThe **mantissa** stores only the fractional part after the leading 1 (which is implicit), so we store `101` (from `1.101`) and pad it with zeros to fill the 52-bit mantissa field.\n:::\n\nIn a dynamic typed language such as Python, the interpreter must operate type checks at runtime for every operation. Then the associated set of instructions for this type should be loaded. Leaving aside typing in Python makes it accessible for beginners. However, this introduces a significant execution time overhead.\n\n::: {.callout-tip icon=false}\n\n## Remember\n\nType inference reduces runtime.\n\n:::\n\n### Hinting the type\n\nIn the Julia programming language, type information is not mandatory. Let us examine, from the *software* perspective, how providing hints to the type inference algorithm allows us to maximize fast executing. First, let's try to understand what the type inference algorithm is trying to do.\n\n::: {#140fa5c1 .cell execution_count=4}\n``` {.julia .cell-code}\na = 2\nb = 3\na+b\n```\n\n::: {.cell-output .cell-output-display execution_count=3}\n```\n5\n```\n:::\n:::\n\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## How does the type inference algorithm proceed on this code snipped?\n\nThe value associated to `a` is the constant integer `2`. The value associated to `b` is the constant integer `3`. By the definition of the `+` operation on two integers, the result is an integer. \n\n:::\n\nThe type of `a` and `b` was found here by human knowledge. The type inference algorithm would have associated `Any` to both `a` and `b`. Indeed, the global scope of the REPL is too dynamic to be optimized. Further in thid section we will, thus, only consider type inference in functions.\n\n::: {#cd69ba82 .cell execution_count=5}\n``` {.julia .cell-code}\nfunction f(a,b)\n    return a+b\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=4}\n```\nf (generic function with 1 method)\n```\n:::\n:::\n\n\nFunction `f` adds input variables `a` and `b`, for which no type hints are provided. Using `@code_llvm`, let us have a look at the associated CPU instructions for input variables of different types. *The instructions generated by the compiler bellow are not in the Assembly language but in the intermediate representation (IR). It could be seen as a hardware agnostic Assembly language.*\n\nFirst, we run `f` to sum two integers, `1+2`.\n\n::: {#278b9c18 .cell execution_count=6}\n``` {.julia .cell-code}\n@code_llvm f(1,2)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n; Function Signature: f(Int64, Int64)\n;  @ In[4]:1 within `f`\ndefine i64 @julia_f_8958(i64 signext %\"a::Int64\", i64 signext %\"b::Int64\") #0 {\ntop:\n;  @ In[4]:2 within `f`\n; ┌ @ int.jl:87 within `+`\n   %0 = add i64 %\"b::Int64\", %\"a::Int64\"\n; └\n  ret i64 %0\n}\n```\n:::\n:::\n\n\nThen, we run `f` to sum two floating point numbers, `1.5+2.5`.\n\n::: {#e5a38ca3 .cell execution_count=7}\n``` {.julia .cell-code}\n@code_llvm f(1.5,2.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n; Function Signature: f(Float64, Float64)\n;  @ In[4]:1 within `f`\ndefine double @julia_f_9040(double %\"a::Float64\", double %\"b::Float64\") #0 {\ntop:\n;  @ In[4]:2 within `f`\n; ┌ @ float.jl:491 within `+`\n   %0 = fadd double %\"a::Float64\", %\"b::Float64\"\n; └\n  ret double %0\n}\n```\n:::\n:::\n\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## How does the type of the input variables influence the type of the returned value?\n\nThe function `f(Int64, Int64)` produces an `i64` in register `%0`. `i64` refers to an integer value stored on 64 bits.\n\n\nSimilarly, `f(Float64, Float64)` produces a `double`. The term `double` is equivalent to `Float64` which refers to a floating point number stored on 64 bits. \n\n:::\n\nLet us have a look at what happens when we mix types in the function call. We run `f` to add an integer `1` and a floating point number `2.5`.\n\n::: {#40c042f9 .cell execution_count=8}\n``` {.julia .cell-code}\n@code_llvm f(1,2.5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n; Function Signature: f(Int64, Float64)\n;  @ In[4]:1 within `f`\ndefine double @julia_f_9044(i64 signext %\"a::Int64\", double %\"b::Float64\") #0 {\ntop:\n;  @ In[4]:2 within `f`\n; ┌ @ promotion.jl:429 within `+`\n; │┌ @ promotion.jl:400 within `promote`\n; ││┌ @ promotion.jl:375 within `_promote`\n; │││┌ @ number.jl:7 within `convert`\n; ││││┌ @ float.jl:239 within `Float64`\n       %0 = sitofp i64 %\"a::Int64\" to double\n; │└└└└\n; │ @ promotion.jl:429 within `+` @ float.jl:491\n   %1 = fadd double %0, %\"b::Float64\"\n; └\n  ret double %1\n}\n```\n:::\n:::\n\n\nWe can see the term *promote* in the output. Thus, the compiler found a type that works as a common ground for the computation.\n\n### Type unstability\n\nIn the previous examples, identifying the type of the returned value was a relatively straightforward process. Let us examine the function `h`, which breaks type stability. We are interested in understanding the reasons behind this problem and techniques to prevent it in our own code, with the aim of achieving optimal performance.\n\n::: {#24eeaccf .cell execution_count=9}\n``` {.julia .cell-code}\nfunction h(x,y)\n  z = x + y\n  if z < 2\n    return z\n  else\n    return Float64(z)\n  end\nend\n```\n\n::: {.cell-output .cell-output-display execution_count=8}\n```\nh (generic function with 1 method)\n```\n:::\n:::\n\n\nUsing `@code_warntype` provides type labels for all the variables of a function. We are particularly interested in the type returned by `h`.\n\n::: {#e08048ac .cell execution_count=10}\n``` {.julia .cell-code}\n@code_warntype h(2,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMethodInstance for h(::Int64, ::Int64)\n  from h(x, y) @ Main In[8]:1\nArguments\n  #self#::Core.Const(Main.h)\n  x::Int64\n  y::Int64\nLocals\n  z::Int64\nBody::Union{Float64, Int64}\n1 ─      (z = x + y)\n│   %2 = z::Int64\n│   %3 = (%2 < 2)::Bool\n└──      goto #3 if not %3\n2 ─ %5 = z::Int64\n└──      return %5\n3 ─ %7 = z::Int64\n│   %8 = Main.Float64(%7)::Float64\n└──      return %8\n\n```\n:::\n:::\n\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## Why does `h` break type stability?\n\nThe type of the output of `h` is said to be `Union{Float64, Int64}`. Whether it is `Int64` or `Float64` does not depend on the type of the input variables. The output type is determined by the value of the `x + y` sum. This information can not be predicted at compile time since it depends on the values for which `h` is called. Thus, the compiler can only restrict the output to `Union{Float64, Int64}`.\n\nNote the color code used in the output of `@code_warntype`. The output for which the type could not be inferred is written in yellow. \n:::\n\nAn other maner to determine type-stability of a function is to use `@inferred`.\n\n::: {#47fb50b3 .cell execution_count=11}\n``` {.julia .cell-code}\nusing Test\n@inferred h(2,5)\n```\n\n::: {.cell-output .cell-output-error}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>return type Float64 does not match inferred return type Union{Float64, Int64}\n\nStacktrace:\n [1] <span class=\"ansi-bold\">error</span><span class=\"ansi-bold\">(</span><span class=\"ansi-bright-black-fg\">s</span>::String<span class=\"ansi-bold\">)</span>\n<span class=\"ansi-bright-black-fg\">   @</span> <span class=\"ansi-bright-black-fg\">Base</span> <span class=\"ansi-bright-black-fg\">./</span><span style=\"text-decoration:underline\" class=\"ansi-bright-black-fg\">error.jl:35</span>\n [2] top-level scope\n<span class=\"ansi-bright-black-fg\">   @</span> <span style=\"text-decoration:underline\" class=\"ansi-bright-black-fg\">In[10]:2</span></pre>\n```\n:::\n\n:::\n:::\n\n\nThe error raised shows this function to be a simple test (yes or no) about inference. \n\n::: {.callout-caution collapse=\"true\" icon=false}\n## Try `@warn_type` and `@inferred` out on previously defined function `f`. What do you get?\n\n::: {#d01108d8 .cell execution_count=12}\n``` {.julia .cell-code}\n@code_warntype f(2,5)\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nMethodInstance for f(::Int64, ::Int64)\n  from f(a, b) @ Main In[4]:1\nArguments\n  #self#::Core.Const(Main.f)\n  a::Int64\n  b::Int64\nBody::Int64\n1 ─ %1 = (a + b)::Int64\n└──      return %1\n\n```\n:::\n:::\n\n\nThe type of the sum of those two integers was correctly inferred: `Body::Int64`. Let us confirm with `@inferred`.\n\n::: {#f290b514 .cell execution_count=13}\n``` {.julia .cell-code}\n@inferred f(2,5)\n```\n\n::: {.cell-output .cell-output-display execution_count=12}\n```\n7\n```\n:::\n:::\n\n\nThe result of the sum is shown (and no error is thrown). This means that the type inference algorithm was successful.\n:::\n\nFrom this analysis we conclude the following rule.\n\n::: {.callout-tip icon=false}\n\n## Remember\n\nThe type of the output of a function should be inferrable from the type of the inputs.\n\n:::\n\n",
    "supporting": [
      "performance_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}