{
  "hash": "dc85d27ffeb42c055cd2bd8bec3c88b6",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: \"Machine Learning\"\npage-layout: article\njupyter: julia-1.11\n---\n\n\n*(Ref. Huda Nasser, [Julia Academy - Data Science](https://juliaacademy.com/p/julia-for-data-science/))*\n\nIn this chapter, we will explore the fundamentals of **machine learning** by working with the **MNIST** dataset — a classic benchmark in computer vision. The MNIST dataset consists of 70,000 handwritten digits (0 through 9), split into 60,000 training images and 10,000 testing images. Each image is a grayscale 28×28 pixel image, making it ideal for experimenting with classification models.\n\nThe Julia language offers powerful packages, including *Flux.jl* (for building neural networks), *MLDatasets.jl* (to access standard datasets) and *OneHotArrays* (for target batching). Throughout, the exercise we will use a set of tools (*Images.jl*, *ImageInTerminal.jl*, *Plots.jl*) in order to make visual cheks along the process. \n\nThe exercise in this chapter will guide you through the following steps:\n\n1. Load and visualize the MNIST dataset;\n2. Preprocess the data for model training;\n3. Build and train a simple machine learning model (here a neural network);\n4. Evaluate the model's performance on unseen data.\n\n## MNIST dataset\n\nThe MNIST dataset can be retrieved from the *MLDatasets.jl* package. Let us load the training dataset.\n\n```julia\nusing MLDatasets\nd_train = MNIST(split=:train)\n```\n\nBefore going further, we checkout what the data we work on looks like. \n\n```julia\nusing Images\nusing ImageInTerminal\ncolorview(Gray,d_train.features[:,:,1])\n```\n\nIt turns out unclear at moments which digit is handwrittent on the image. To clarify this, we can look at the label associated to the image.\n\n```julia\nd_train.targets[1]\n```\n\n## Neural network\n\nA neural network is a type of machine learning model inspired by the structure and function of the human brain. It is composed of layers of interconnected nodes called neurons, which work together to process data, recognize patterns, and make predictions.\n\nAt its core, a neural network learns to approximate complex functions by adjusting the weights and biases of these connections based on the data it sees.\n\nA standard neural network for the MNIST dataset to start of with has the following structure:\n\n**Input (784)** ⟶ **Dense (32)** ⟶ **ReLU** ⟶ **Dense (10)** ⟶ **Softmax** ⟶ **Output (Digit 0–9)**\n\nThe 28-by-28 gray-scale images are flattened into a 784 element vector. No activation function is applied at this stage — the input is just passed to the next layer. The input data is passed to a 32-neuron hidden layer which computes a weighted sum of the inputs, adds a bias, and passes the result through an activation function, here ReLU (Rectified Linear Unit) to introduce non-linearity. The output layer has 10 neurons to be consistent with the 10 possible targets (0 through 9). This being a classification task of the handwritten digit, we use a Softmax activation function to convert the outputs into probabilities that sum to 1.\n\n### Preprocess the dataset\n\nThe neural network we will be using in this exercise requires a 1D-vector of length 784 input. Let us flatten the matrices representing the images of our dataset using *Flux.jl*.\n\n```julia\nusing Flux\nv_train = Flux.flatten(d_train.features)\nnothing #| hide\n```\n\nNow we make use of *OneHotArrays.jl* to transform the target array to vectors of 10 elements with 1 at the index of the target digit.\n\n```julia\nusing OneHotArrays\nY = onehotbatch(d_train.targets,0:9)\n```\n\n### Set-up the neural network\n\nHere we simply translate the neural network schematic to Julia language code.\n\n```julia\nm = Chain(\n          Dense(28*28,32,relu),\n          Dense(32,10),\n          softmax\n         )\nnothing #| hide\n```\n\nWhat happens if we apply this neural network to one of the images?\n\n```julia\nm(v_train[:,1])\n```\n\nThe network is not able to determine which digit is associated to this image. The weights and biases of the connections between the neurons have not get been adjusted since the neurol network we created as not yet been *trained*. \n\n## Training\n\nYou can start by having a look at the training function within *Flux.jl* in the following way. \n\n```julia\n#| output: false \n? Flux.train!\n```\n\n::: {.callout-warning}\nTake care when changing package version to have a look at the major changes. For instance, from version 0.14 of *Flux.jl* on the syntax for `Flux.train!` changed. Indeed, it went from `Flux.train!(loss, params(model), data, opt)` to `Flux.train!(loss, model, data, opt_state)  # using the new \"setup\" from Optimisers.jl`.\n:::\n\nWhen a neural network makes predictions (like classifying an image as a \"3\" instead of a \"7\"), we need a way to measure the difference between the predicted output and the actual (true) target.\n\nThe loss function provides this measure. It returns a numerical value that represents the \"error\" — the higher the value, the worse the prediction. Since we have a classification problem in this exercise, a typical loss choice is the cross-entropy loss.\n\n```julia\nloss(m,x, y) = Flux.Losses.crossentropy(m(x),y)\nnothing #| hide\n```\n\nTo properly train the neural network we wish to minimize the loss function. To do so, we will be using a variant of gradient descent called ADAM.\n\n```julia\noptimizer = Flux.setup(Adam(), m)\nnothing #| hide\n```\n\nWhen training a neural network, we often need to go over the training data multiple times. Each full pass over the training data is called an epoch.\n\n```julia\nusing IterTools: ncycle\ndataset = ncycle([(v_train, Y)], 200)\nnothing #| hide\n```\n\nThe `dataset` storage constructed in the cell above tells us to train for 200 epochs. This means that the network will see the training data 200 times.\n\nLet's **train** the neural network now!\n\n```julia\nFlux.train!(loss, m, dataset, optimizer)\n```\n\nSo, does it work better than previously on our first image?\n\n```julia\ntst = m(v_train[:,1])\ncls = argmax(tst)-1\ntgt = d_train.targets[1]\nprintln(\"Image classified as \", cls, \" with target \", tgt, \".\")\n```\n\nGreat! Let us now have a look under the hood of `Flux.train!`. What is happening in the training loop?\n\n- Take a subset of input data with associated targets: a batch;\n- Determine whether the model `m` predicts well the targets: use the loss function;\n- Find out which direction each model parameter should move to: compute the gradient of the loss with respect to each parameter;\n- Adjust the parameters using the gradients and an optimizer.\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## Write manually the training loop based on above stated steps using *Flux.jl* utilities like `gradient`, `Flux.trainable` and `Flux.Optimise.update!`.\n\n\n```julia\n#| eval: false\nopt = Flux.setup(Adam(), m)\nloss(x, y) = Flux.Losses.crossentropy(x, y)\n\n# Training loop\nfor epoch in 1:200\n    grads = Flux.gradient(m) do model\n      result = model(v_train)\n      loss(result, Y)\n    end\n    Flux.update!(opt, m, grads[1])\n    println(\"Epoch $epoch | Loss: \", loss(m(v_train), Y))\nend\n```\n\nWhat happens when you replace the `Adam()` optimizer by a standard `Descent()`? Or the loss by a Mean Square Error (MSE)? Can you find the available loss functions in the *Flux.jl* package documentation?\n:::\n\n## Testing\n\nWe can now evaluate our trained neural network on unseen data, the so-called test dataset.\n\n```julia\nd_test = MNIST(split=:test)\nfor i in 1:10\n   b = d_test.features[:,:,i]\n   v_b = reshape(b,784)\n   a = m(v_b)\n   r = argmax(a)-1\n   println(\"Image classified as \", r, \" with target \", d_test.targets[i], \".\")\nend\nnothing #| hide\n```\n\nThe results seem pretty good at first glance.\n\n::: {.callout-caution collapse=\"true\" icon=false}\n## Generate a violin plot of the predicted probability associated to the target digit for the images in the test dataset. Use the *Plots.jl* documentation to do so. \n\nAs you may have noticed, the *Plots.jl* documentation is build upon examples. Many recent documentations begin with examples before moving on to the general definition.  \n\n```julia\nusing StatsPlots\n\n# Array with probabilities associated to target digits for each image\nval = [m(reshape(d_test.features[:,:,i],784))[d_test.targets[i]+1] for i in 1:length(d_test.targets)]\n\ngroup = fill(\"MNIST-Test\", length(val))\n\nviolin(group, val, legend=false, title=\"Violin Plot\", ylabel=\"Target digit probability\")\n```\n\nWhat do you think about the models performance based on the violin plot generated by the lines of code above?\n:::\n\n",
    "supporting": [
      "machine_learning_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}