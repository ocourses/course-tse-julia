---
title: "The Newton Method and its Applications"
page-layout: article
jupyter: julia-1.11
---

```{julia}
#| echo: false
include("activate.jl")
```

The Newton method, also known as Newton-Raphson, is a powerful iterative technique widely used for solving nonlinear equations and optimization problems. In the context of nonlinear equations, the method seeks a root of a function $f(x) = 0$ by using the derivative information to iteratively refine an initial guess. For optimization, the method can be adapted to minimize a function by seeking the stationary points where its gradient vanishes. While Newton's method is highly efficient, requiring quadratic convergence under certain conditions, it also relies on the computation of derivatives, making it less suitable for some problems.

To build up to Newton's method, we first introduce the [bisection method](https://en.wikipedia.org/wiki/Bisection_method). Unlike Newton's method, the bisection method does not require derivatives, making it an excellent starting point for understanding root-finding techniques. 

## The Bisection Method

The bisection method is an iterative numerical technique for finding a root of a continuous function $f(x)$ on a closed interval $[a, b]$. The method assumes that $f(a)$ and $f(b)$ have opposite signs, which guarantees, by the Intermediate Value Theorem, that there is at least one root in $[a, b]$. The procedure works by repeatedly halving the interval and selecting the subinterval where the sign change occurs. 

### Exercise: Implement the Bisection Method

1. **Objective**: Implement the bisection method in Julia and analyze its behavior.

2. **Function Signature**: Define a function `bisection(f, a, b; tol=1e-12, max_iter=100)` that takes:
   - A continuous function `f`.
   - The interval bounds `a` and `b`.
   - An optional tolerance `tol` for stopping criteria.
   - An optional maximum number of iterations `max_iter`.

The function should return:

   - An approximation of the root.
   - The number of iterations performed.

Stopping criteria for the bisection method include:

| **Condition**  | **Description** |
|----------------|-----------------|
| The length of the interval $[a, b]$    | $\leq \texttt{tol}$: The interval has shrunk to a size less than or equal to the tolerance. |
| Function value at midpoint of $[a, b]$ | $\texttt{abs}(f((a + b)/2)) \leq \texttt{tol}$: The function value at the midpoint is close enough to zero. |
| Maximum number of iterations reached   | The bisection algorithm terminates after reaching the predefined maximum iterations. |

3. **Example Problem**: Use the bisection method to find the root of $f(x) = \cos(x) - x$ on the interval $[0, 1]$.

4. **Analysis**:
   - Record the approximate root at each iteration.
   - Calculate the absolute error $|x_k - x^*|$ where $x^* = 0.7390851332151607$ is the true root.
   - Plot the error on a logarithmic scale and determine the [order of convergence](https://en.wikipedia.org/wiki/Rate_of_convergence).

5. **Discussion**:
   - Observe the behavior of the error as the iterations progress.
   - Conclude that the bisection method has a **linear convergence order**.

::: {.callout-tip collapse="true" icon=false}
### Hint for Implementation

- Use a loop to halve the interval and check the signs of $f(a)$, $f(b)$, and $f(\text{midpoint})$.
- Stop the iterations when $|b - a| / 2 \leq \text{tol}$ or the maximum number of iterations is reached.
- Handle cases where the initial interval does not satisfy the conditions for the method.
:::

::: {.callout-caution collapse="true" icon=false}
### Correction for the Bisection Method
```{julia}
function bisection(f, a, b; tol=1e-12, max_iter=100)
    # Check if the initial interval is valid
    if f(a) * f(b) > 0
        error("The function must have opposite signs at the endpoints a and b.")
    end
    
    # Initialize variables
    mid = (a + b) / 2
    iter_count = 0
    
    while (b - a) / 2 > tol && iter_count < max_iter
        iter_count += 1
        mid = (a + b) / 2
        
        # Check if the midpoint is a root
        if abs(f(mid)) ≤ tol
            return mid, iter_count
        elseif f(a) * f(mid) < 0
            b = mid  # Root is in [a, mid]
        else
            a = mid  # Root is in [mid, b]
        end
    end
    
    # Return the midpoint as the approximate root
    return mid, iter_count
end

# Example usage
f(x) = cos(x) - x
root, iterations = bisection(f, 0, 1)
println("Approximate root: $root")
println("Evaluated function at the root: $(f(root))")
println("Number of iterations: $iterations", "\n")

# Exact root for comparison
exact_root = 0.7390851332151607
println("Exact root: $exact_root")
println("Evaluated function at the exact root: $(f(exact_root))")
println("Absolute error: $(abs(root - exact_root))")
println("Relative error: $(abs(root - exact_root) / exact_root)")
```
:::

::: {.callout-caution collapse="true" icon=false}
### Correction for Analysis

To analyze the convergence of the bisection method for $f(x) = \cos(x) - x$ on the interval $[0, 1]$, we compute the approximate root at each iteration and calculate the absolute error. Using the exact root $x^* \approx 0.7390851332151607$, we determine the error $|x_k - x^*|$. We also plot the error on a logarithmic scale to observe the convergence behavior.

Below is the Julia code for the analysis:

```{julia}
using Plots

function bisection_analysis(f, a, b; tol=1e-12, max_iter=100, true_root=nothing)
    if f(a) * f(b) > 0
        error("The function must have opposite signs at the endpoints a and b.")
    end

    midpoints = []
    errors = []
    iter_count = 0
    mid = (a + b) / 2
    
    while (b - a) / 2 > tol && iter_count < max_iter
        iter_count += 1
        mid = (a + b) / 2
        push!(midpoints, mid)
        
        if !isnothing(true_root)
            push!(errors, abs(mid - true_root))
        end

        if abs(f(mid)) ≤ tol
            break
        elseif f(a) * f(mid) < 0
            b = mid
        else
            a = mid
        end
    end
    
    return mid, iter_count, midpoints, errors
end

# Exact root for reference
true_root = 0.7390851332151607
f(x) = cos(x) - x
root, iterations, midpoints, errors = bisection_analysis(f, 0, 1, true_root=true_root)

# Print the approximate root and error
println("Approximate root: $root")
println("Number of iterations: $iterations")
println("Final error: $(errors[end])")

# Plot the errors on a logarithmic scale
plt = plot(1:iterations, errors, yscale=:log10, xlabel="Iteration", ylabel="Absolute Error", 
    title="Convergence of the Bisection Method", label="Computed Error")

# Compute by linear regression the convergence rate
using Polynomials
line = fit(1:iterations, log10.(errors), 1)
println("Convergence rate: $(10^(line.coeffs[2]))")

# add the linear regression line to the plot
plot!(plt, 1:iterations, 10 .^ line.(1:iterations), label="Linear Regression")
```
:::


## The Newton Method

The **Newton Method**, or **Newton-Raphson Method**, is an iterative numerical technique for finding roots of a nonlinear equation $f(x) = 0$. Unlike the bisection method, Newton's method leverages the derivative of $f(x)$ to achieve a much faster rate of convergence under suitable conditions. Specifically, it exhibits **quadratic convergence**, meaning the number of accurate decimal places roughly doubles at each iteration once close to the root.

### Newton's Iteration Formula

Given an initial guess $x_0$, the Newton iteration is defined as:

$$
x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)},
$$

where $f'(x_k)$ is the derivative of $f(x)$ evaluated at $x_k$. The method relies on the approximation of $f(x)$ by its tangent line at $x_k$. The next iterate $x_{k+1}$ is where this tangent line intersects the $x$-axis.

### Exercise: Apply Newton's Method to Find a Root

1. **Objective**: Use the Newton method to find the root of 
    $$f(x) = \cos(x) - x,$$ 
    starting from the initial iterate $x_0 = 0$.

2. **Function Signature**: Define a function `newton(f, df, x0; tol=1e-12, max_iter=100)` that takes:
   - A continuous function `f`.
   - Its derivative `df`.
   - The initial guess `x0`.
   - An optional tolerance `tol` for stopping criteria.
   - An optional maximum number of iterations `max_iter`.

The function should return:

   - The approximate root.
   - The number of iterations performed.

The stopping criteria for the Newton method include:

| **Condition**                                | **Description**                                                                 |
|----------------------------------------------|---------------------------------------------------------------------------------|
| Difference between consecutive iterations    | $\texttt{abs}(x_{k+1} - x_k) \leq \texttt{tol}$: The method converges when the change between successive approximations is less than or equal to the tolerance. |
| Maximum number of iterations reached         | The Newton method terminates after reaching the predefined maximum iterations.  |


1. **Example Problem**: Use the Newton method to find the root of $f(x) = \cos(x) - x$ starting at $x_0 = 0$.

2. **Analysis**:
   - Record the approximate root at each iteration.
   - Calculate the absolute error $|x_k - x^*|$ where $x^* \approx 0.7390851332151607$ is the true root.
   - Plot the error on a logarithmic scale and determine the **order of convergence**.

::: {.callout-tip collapse="true" icon=false}
### Hint for Implementation

- Use the Newton iteration formula to update the guess at each iteration.
- Stop the iterations when $|x_{k+1} - x_k| \leq \text{tol}$ or the maximum number of iterations is reached.
:::

::: {.callout-caution collapse="true" icon=false}
### Correction for the Newton Method

```{julia}
function newton(f, df, x0; tol=1e-12, max_iter=100)
    xk = x0
    iter_count = 0

    while iter_count < max_iter
        iter_count += 1
        x_next = xk - f(xk) / df(xk)

        # Check if the difference is within the tolerance
        if abs(x_next - xk) < tol
            return x_next, iter_count
        end

        xk = x_next
    end

    return xk, iter_count
end

# Example usage
f(x) = cos(x) - x
df(x) = -sin(x) - 1
x0 = 0
root, iterations = newton(f, df, x0)

println("Approximate root: $root")
println("Evaluated function at the root: $(f(root))")
println("Number of iterations: $iterations", "\n")

# Exact root for comparison
exact_root = 0.7390851332151607
println("Exact root: $exact_root")
println("Evaluated function at the exact root: $(f(exact_root))")
println("Absolute error: $(abs(root - exact_root))")
println("Relative error: $(abs(root - exact_root) / exact_root)")
```
:::

::: {.callout-caution collapse="true" icon=false}
### Correction for Analysis

To analyze the convergence of the Newton method for $f(x) = \cos(x) - x$, we compute the approximate root at each iteration and calculate the absolute error. Using the exact root $x^* \approx 0.7390851332151607$, we determine the error $|x_k - x^*|$. We also plot the error on a logarithmic scale to observe the convergence behavior.

Below is the Julia code for the analysis:

```{julia}
using Plots
using Polynomials

function newton_analysis(f, df, x0; tol=1e-12, max_iter=100, true_root=nothing)
    xk = x0
    iter_count = 0
    approximations = []
    errors = []

    while iter_count < max_iter
        iter_count += 1
        x_next = xk - f(xk) / df(xk)
        push!(approximations, x_next)

        if !isnothing(true_root)
            push!(errors, abs(x_next - true_root))
        end

        if abs(x_next - xk) < tol
            break
        end

        xk = x_next
    end

    return approximations, errors
end

# Exact root for reference
true_root = 0.7390851332151607
f(x) = cos(x) - x
df(x) = -sin(x) - 1
x0 = 0
approximations, errors = newton_analysis(f, df, x0, true_root=true_root)

# Exclude zero values for quadratic fitting
non_zero_errors = filter(e -> e > 0, errors)  # Only non-zero errors
indices_non_zero = findall(e -> e > 0, errors)  # Indices of non-zero errors

if length(non_zero_errors) > 2
    # Fit a quadratic polynomial to the log of non-zero errors
    p = fit(indices_non_zero, log10.(non_zero_errors), 2)  # Fit quadratic to log10 of errors
    println("Quadratic fit: $p")

    # Replace zero errors with the values predicted by the quadratic fit
    for i in 1:length(errors)
        if errors[i] == 0
            errors[i] = 10^p(i)  # Replace with the fitted value at the iteration
        end
    end
end

# Print the approximate root and error
println("Approximate root: $(approximations[end])")
println("Final error: $(errors[end])")

# Create the plot
plt = plot(1:length(errors), errors, yscale=:log10, xlabel="Iteration", ylabel="Absolute Error", 
    title="Convergence of the Newton Method", label="Computed Error")

# Add the fitted quadratic to the plot
iterations = 1:length(errors)
fitted_values = 10 .^ p.(iterations)
plot!(plt, iterations, fitted_values, label="Quadratic Fit", linestyle=:dash)

# Show the plot
display(plt)
```
:::
